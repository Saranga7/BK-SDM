### bk_base:
Total number of parameters: 579,384,964
Number of trainable parameters: 579,384,964


### bk_small:
Total number of parameters: 482,346,884
Number of trainable parameters: 482,346,884


### bk_tiny:
Total number of parameters: 323,384,964
Number of trainable parameters: 323,384,964







### my_bk_1:

bk_tiny with identity activations instead of silu

### my_bk_2:

bk_tiny with attnetion heads halved (4)
Total number of parameters: 323,384,964
Number of trainable parameters: 323,384,964




### my_bk_3:

bk_tiny with number of channels and cross_attention_dim halved

### my_bk_2:

bk_tiny with number of channels halved

### my_bk_3:

bk_tiny with number of channels and cross_attention_dim halved


### my_bk_4:

Quantized model
https://pytorch.org/docs/stable/quantization.html